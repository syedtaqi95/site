---
title: "Designing Data-Intensive Applications"
tags:
- books
- distributed systems
published: 14 October 2024
---

My notes on the quintessential distributed systems book
[Designing Data-Intensive Applications](https://www.oreilly.com/library/view/designing-data-intensive-applications/9781491903063/) 
by Dr. Martin Kleppmann.

![Designing Data-Intensive Applications book cover.](../../assets/ddia-book-cover.png)

# Table of Contents

# Part 1: Foundations of Data Systems

## Reliable, Scalable and Maintainable Applications

TBC.

## Data Models and Query Languages

Most software applications are built by layering one data model over another. For
each layer, the key question is: how is it _represented_ in terms of the next layer?
For example:

1. Application developers model the real world into data structures, 
and APIs (algorithms) that manipulate those data structures.
2. When you want to store these data structures, you convert them into general-purpose
data models such as JSON, XML, SQL tables, graph models etc.
3. The database developers decided on a way to convert these data models in terms of
bytes in memory, on disk, or on a network. They also decided on a way to do operations 
(like CRUD) on this data.
4. At the lowest level, hardware engineers represent those bytes in terms of 
electrical currents, magnetic fields, light pulses etc.

### Relational model vs document model

Relational model: data is organized into relations (called tables in SQL), 
where each relation is an unordered collection of tuples (rows in SQL).

Originally used for business data processing in the 60s and 70s:
- Transaction processing: sales transactions, airline reservations.
- Batch processing: customer invoicing, payroll, reporting.

The goal of the relational model was to abstract away the internal representation
of the data behind a cleaner interface.

#### The Birth of NoSQL

NoSQL started in the 2010s to overcome some limitations of the relational model:
- Scalability, including very large datasets or very high write throughput
- FOSS over commercial software
- Specialised query operations that are not supported by SQL
- Desire for a more dynamic and expressive schema or data model

#### The Object-Relational Mismatch

Modern applications have an _impedance mismatch_ between the object-oriented 
application code and the relational data in databases. ORMs can help reduce the
differences but not completely eliminate them.

For data structures like self-contained documents (e.g. resumes), a JSON 
representation is more suitable than a relational model due to better locality
and the one-to-many relationships (i.e. a tree structure) is explicit in JSON.

#### Many-to-One and Many-to-Many Relationships

Using an ID vs a string to store data is a question of duplication. When using
IDs, the data is only stored in one place, and everything refers to the ID. When
storing the text directly, we duplicate the human-meaningful information in 
every record. Removing this duplication is the key idea behind _normalisation_
in databases.

Data normalisation requires many-to-one relationships, which don't fit nicely
into the document model. In relational databases, we use IDs and join the tables 
because joins are easy. In document databases, joins are not needed for 
one-to-many tree structures, and support for joins is weak. 

Moreover, even when document models start off join-free, they have a tendency 
of becoming more interconnected as features are added to applications. In this 
case, if joins are not supported by the document database, then they are 
emulated in application code through multiple queries.

#### Are Document Databases Repeating History?

The most popular database for business data processing in the 1970s was IBM's
_Information Management System_ (IMS). It used the hierarchical model, which 
represents all data as a tree of records nested within records, similar to JSON.

Like document databases, IMS worked well for one-to-many relationships but it 
made many-to-many relationships difficult and did not support joins.

**Network model**

The network/CODASYL model was a generalisation of the hierarchical model, i.e.
a record could have multiple parents. This allowed many-to-one and many-to-many
relationships to be modelled.

The links between records were like pointers. The only way of accessing a record
was to follow a path from a root record along these chains of links, called an
_access path_.

A query in CODASYL was done by moving a cursor through the database by iterating
over lists of records and following access paths. If you didn't have a path to
the data you wanted, you were in a difficult position.

**The relational model**

In contrast, the relational model laid out all data in the open: a relation 
(table) is simply a collection of tuples (rows). There are no nested structures 
or access paths.

The query optimiser automatically decides the execution order of each part of the
query, so the application developer does not need to think about them.

If you wanted to query your data in new ways, you simply declare a new index,
and queries will automatically use whichever indexes are most appropriate. Hence,
the relational model made it much easier to add new features to applications.

**Comparison to document databases**

The network model is similar to document databases in one aspect: storing nested
records within the parent. They are different in that document databases use
document references for many-to-one and many-to-many relationships which are 
resolved at read time using joins or follow-up queries, similar to relational 
models.

#### Relational vs Document Databases Today

The main advantages of document databases are schema flexibility, better performance
due to locality, and that for some applications it is closer to the data structures
used by the application.

The main advantages of relational models are better support for joins, and 
many-to-one and many-to-many relationships.

**Which data model leads to simpler application code?**

If the data has a document-like structure, then it's best to use a document model.
The relational technique of _shredding_ (splitting a document structure into 
multiple tables) can lead to cumbersome schemas and unnecessarily complicated code.

The poor support for joins may or may not be a problem, for example, analytics
applications using a document database to record events with timestamps will be 
fine.

However, they are less appealing for modelling many-to-many relationships. The
code for joins moves into application code, increasing complexity, and is 
usually slower than specialised code inside the database.

For highly interconnected data, the document model is bad, the relational model
is acceptable, and graph models are the most natural.

**Schema flexibility in the document model**

Document databases have a _schema-on-read_ while relational models have a 
_schema-on-write_.

The difference is notable in situations where applications want to change the
format of its data, for example going from storing a user's full name to
storing their first and last names separately. 

In document databases, you could simply start writing new documents with the new
schema and handle reads of the old schema in application code.

In relational models, you would have to perform a database migration.

The schema-on-read approach is more advantageous when:
- There are many different types of objects and it is impractical to put each
type of object in its own table
- The structure of the data is determined by external systems outside the 
control of the application developers.

**Data locality for queries**

If the entire document needs to be accessed, then there is a performance
advantage to this _storage locality_. However, this only applies if large parts
of the document are required at the same time. Only modifications that don't 
change the encoded size of a document can easily be performed in-place. Hence,
it is recommended to keep documents fairly small and avoid writes that increase
the size of the document. This significantly reduces the set of situations where
document databases are useful.

Google's Spanner database offers this same locality in the relational model, by 
allowing the schema to declare that a table's rows should be interleaved within
a parent table. The _column-family_ concept in Bigtable data model has a similar
purpose of managing locality.

**Convergence of document and relational databases**

Modern relational databases support JSON documents. Also document databases like
RethinkDB support relational-like joins in their query languages. A hybrid approach
is a good route for databases to take in the future.

### Query Languages for Data

SQL introduced a declarative syntax for query languages, whereas older query
languages like CODASYL and IMS use an imperative style.

Some advantages of declarative query languages over imperative alternatives:
- A declarative query language is typically more concise and easier to work with
than imperative ones. 
- It abstracts away implementation details of the database, which makes it easier
for database systems to introduce performance improvements without requiring any
changes to queries.
- Declarative syntax allows for parallel execution of queries. Imperative syntax
makes parallel execution very difficult as it specifies the order of instructions.

#### Declarative Queries on the Web

CSS/XSL use a declarative syntax for styling HTML elements, wherease the 
imperative approach would use (ugly and terse) JavaScript code.

#### MapReduce Querying

_MapReduce_ is a programming model for processing large amounts of data in bulk
across many machines, popularised by Google.

It is neither fully declarative neither fully imperative but somewhere in between:
the logic of the query is expressed with snippets of code, which are called
repeatedly by the processing framework.

For example, a PostgreSQL query like:

```sql
SELECT date_trunc('month', observation_timestamp) AS observation_month,
 sum(num_animals) AS total_animals
FROM observations
WHERE family = 'Sharks'
GROUP BY observation_month;
```

Can be expressed as a MongoDB MapReduce query like so:

```typescript
db.observations.mapReduce(
  function map() {
    var year = this.observationTimestamp.getFullYear();
    var month = this.observationTimestamp.getMonth() + 1;
    emit(year + "-" + month, this.numAnimals);
  },
  function reduce(key, values) {
    return Array.sum(values);
  },
  {
    query: { family: "Sharks" },
    out: "monthlySharkReport"
  }
);
```

The `map` and `reduce` functions must be pure functions, i.e. they cannot perform
additional database queries and they cannot have any side effects. This allows
them to be run in any order, anywhere, and be rerun on failure.

A usability problem with MapReduce is that you have to write two carefully 
coordinated JavaScript functions. Moreover, a declarative query language offers
more opportunities for a query optimiser to improve the performance of a query.

Hence, MongoDB added a declarative query language called `aggregation pipeline`,
which looks like this:

```typescript
db.observations.aggregate([
  { $match: { family: "Sharks" } },
  { $group: {
    _id: {
      year: { $year: "$observationTimestamp" },
      month: { $month: "$observationTimestamp" }
    },
    totalAnimals: { $sum: "$numAnimals" }
  } }
]);
```
